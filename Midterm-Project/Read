ECE 411, Computational Graphs for Machine Learning
Professor Chris Curro

A Tensorflow Implementation of Eve that Improves Upon Adam SGD Optimization
By Frank Longueira

	The paper “Improving Stochastic Gradient Descent” introduces a new method for altering the learning rate of a learning algorithm that uses stochastic gradient descent. This method of altering the learning rate takes into account feedback information from the objective (loss) function that is being optimized. By looking at the value of the cost function at a given iteration & the value of the cost function at the previous iteration, a new parameter d is computed taking into account a smoothing factor and numerical stability. An effective learning rate is produced by taking the current learning rate and dividing by d. The paper specifically employs this methodology to Adam which is state of the art with regards to stochastic gradient descent optimization. The resulting algorithm is named Eve. Eve is shown to improve upon Adam for different datasets and network structures.
	In this midterm project, Eve was implemented using Tensorflow. This implementation expanded upon a current implementation of Adam by OpenAI. This implementation of Eve was then used to train a well-known, 2-layer convolutional network that achieves over 99% on the MNIST dataset. Convergence of the loss function was compared with that of Adam. In this specific case, Eve actually did not do better than Adam in terms of convergence and generalization. Adam resulted in a network that achieved 99.5% accuracy on the MNIST test set, while Eve resulted in only 99.2% accuracy. In addition to studying Eve’s convergence, the scaling parameter d was tracked during the optimization process. Similar to results in the paper, d first decreased drastically (from 1 to about 0.25) then increased and maintained stability around 0.5 by the end of the training session.
	To conclude, the concept introduced in this paper is quite intuitive and seems to improve upon state-of-the art with little tradeoff. The computation of d is inexpensive and the three new hyper parameters that are introduced for numerical stability & smoothing seem to not need to be changed for a wide variety of applications. In terms of future work for the final project, I would like to extend the Eve algorithm. Initial thoughts involve possibly looking at relative change of the cost function over longer durations & including more samples than just one previous point. This project definitely helped me better understand using Tensorflow as a framework, especially the use of control-flow statements and generation of new operations that are needed for the computation graph. In addition, it was a great experience deciphering a paper and implementing the key idea from it.
